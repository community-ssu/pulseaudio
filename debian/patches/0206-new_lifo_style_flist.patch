diff -ur pulseaudio-0.9.15.fix.orig/src/pulsecore/flist.c pulseaudio-0.9.15.fix/src/pulsecore/flist.c
--- pulseaudio-0.9.15.fix.orig/src/pulsecore/flist.c	2012-12-17 06:14:21.000000000 -0500
+++ pulseaudio-0.9.15.fix/src/pulsecore/flist.c	2013-01-11 05:09:14.399625881 -0500
@@ -2,6 +2,10 @@
   This file is part of PulseAudio.
 
   Copyright 2006-2008 Lennart Poettering
+  Copyright (C) 2009 Nokia Corporation and/or its subsidiary(-ies).
+  Copyright (C) 2012 Canonical Ltd.
+
+  Contact: Jyri Sarha <Jyri.Sarha@nokia.com>
 
   PulseAudio is free software; you can redistribute it and/or modify
   it under the terms of the GNU Lesser General Public License as
@@ -27,191 +31,139 @@
 
 #include <pulsecore/atomic.h>
 #include <pulsecore/log.h>
-#include <pulsecore/thread.h>
 #include <pulsecore/macro.h>
 #include <pulsecore/core-util.h>
 #include <pulsecore/macro.h>
 
 #include "flist.h"
 
-/* Algorithm is not perfect, in a few corner cases it will fail to pop
- * from the flist although it isn't empty, and fail to push into the
- * flist, although it isn't full.
- *
- * We keep a fixed size array of entries, each item is an atomic
- * pointer.
- *
- * To accelerate finding of used/unused cells we maintain a read and a
- * write index which is used like a ring buffer. After each push we
- * increase the write index and after each pop we increase the read
- * index.
- *
- * The indexes are incremented atomically and are never truncated to
- * the buffer size. Instead we assume that the buffer size is a power
- * of two and that the truncation can thus be done by applying a
- * simple AND on read.
- *
- * To make sure that we do not look for empty cells indefinitely we
- * maintain a length value which stores the number of used cells. From
- * this value the number of unused cells is easily calculated. Please
- * note that the length value is not updated atomically with the read
- * and write index and might thus be a few cells off the real
- * value. To deal with this we always look for N_EXTRA_SCAN extra
- * cells when pushing/popping entries.
- *
- * It might make sense to replace this implementation with a link list
- * stack or queue, which however requires DCAS to be simple. Patches
- * welcome.
- *
- * Please note that this algorithm is home grown.*/
-
 #define FLIST_SIZE 128
-#define N_EXTRA_SCAN 3
 
-/* For debugging purposes we can define _Y to put and extra thread
- * yield between each operation. */
+/* Atomic table indices contain
+   sign bit = if set, indicates empty/NULL value
+   tag bits (to avoid the ABA problem)
+   actual index bits
+*/
+
+/* Lock free single linked list element. */
+struct pa_flist_elem {
+    pa_atomic_t next;
+    pa_atomic_ptr_t ptr;
+};
 
-#ifdef PROFILE
-#define _Y pa_thread_yield()
-#else
-#define _Y do { } while(0)
-#endif
+typedef struct pa_flist_elem pa_flist_elem;
 
 struct pa_flist {
     unsigned size;
-    pa_atomic_t length;
-    pa_atomic_t read_idx;
-    pa_atomic_t write_idx;
+
+    pa_atomic_t current_tag;
+    int index_mask;
+    int tag_shift;
+    int tag_mask;
+
+    /* Stack that contains pointers stored into free list */
+    pa_atomic_t stored;
+    /* Stack that contains empty list elements */
+    pa_atomic_t empty;
+    pa_flist_elem table[];
 };
 
-#define PA_FLIST_CELLS(x) ((pa_atomic_ptr_t*) ((uint8_t*) (x) + PA_ALIGN(sizeof(struct pa_flist))))
+/* Lock free pop from linked list stack */
+static pa_flist_elem *stack_pop(pa_flist *flist, pa_atomic_t *list) {
+    pa_flist_elem *popped;
+    int idx;
+    pa_assert(list);
+
+    do {
+        idx = pa_atomic_load(list);
+        if (idx < 0)
+            return NULL;
+        popped = &flist->table[idx & flist->index_mask];
+    } while (!pa_atomic_cmpxchg(list, idx, pa_atomic_load(&popped->next)));
+
+    return popped;
+}
+
+/* Lock free push to linked list stack */
+static void stack_push(pa_flist *flist, pa_atomic_t *list, pa_flist_elem *new_elem) {
+    int tag, newindex, next;
+    pa_assert(list);
+
+    tag = pa_atomic_inc(&flist->current_tag);
+    newindex = new_elem - flist->table;
+    pa_assert(newindex >= 0 && newindex < (int) flist->size);
+    newindex |= (tag << flist->tag_shift) & flist->tag_mask;
+
+    do {
+        next = pa_atomic_load(list);
+        pa_atomic_store(&new_elem->next, next);
+    } while (!pa_atomic_cmpxchg(list, next, newindex));
+}
 
 pa_flist *pa_flist_new(unsigned size) {
     pa_flist *l;
+    unsigned i;
 
     if (!size)
         size = FLIST_SIZE;
 
-    pa_assert(pa_is_power_of_two(size));
-
-    l = pa_xmalloc0(PA_ALIGN(sizeof(pa_flist)) + (sizeof(pa_atomic_ptr_t) * size));
+    l = pa_xmalloc0(sizeof(pa_flist) + sizeof(pa_flist_elem) * size);
 
     l->size = size;
 
-    pa_atomic_store(&l->read_idx, 0);
-    pa_atomic_store(&l->write_idx, 0);
-    pa_atomic_store(&l->length, 0);
-
+    while (1 << l->tag_shift < (int) size)
+        l->tag_shift++;
+    l->index_mask = (1 << l->tag_shift) - 1;
+    l->tag_mask = INT_MAX - l->index_mask;
+
+    pa_atomic_store(&l->stored, -1);
+    pa_atomic_store(&l->empty, -1);
+    for (i=0; i < size; i++) {
+        stack_push(l, &l->empty, &l->table[i]);
+    }
     return l;
 }
 
-static unsigned reduce(pa_flist *l, unsigned value) {
-    return value & (l->size - 1);
-}
-
 void pa_flist_free(pa_flist *l, pa_free_cb_t free_cb) {
     pa_assert(l);
 
     if (free_cb) {
-        pa_atomic_ptr_t*cells;
-        unsigned idx;
-
-        cells = PA_FLIST_CELLS(l);
-
-        for (idx = 0; idx < l->size; idx ++) {
-            void *p;
-
-            if ((p = pa_atomic_ptr_load(&cells[idx])))
-                free_cb(p);
-        }
+        pa_flist_elem *elem;
+        while((elem = stack_pop(l, &l->stored)))
+            free_cb(pa_atomic_ptr_load(&elem->ptr));
     }
 
     pa_xfree(l);
 }
 
-int pa_flist_push(pa_flist*l, void *p) {
-    unsigned idx, n, len;
-    pa_atomic_ptr_t*cells;
-
+int pa_flist_push(pa_flist *l, void *p) {
+    pa_flist_elem *elem;
     pa_assert(l);
     pa_assert(p);
 
-    cells = PA_FLIST_CELLS(l);
-
-    n = len = l->size + N_EXTRA_SCAN - (unsigned) pa_atomic_load(&l->length);
-
-    _Y;
-    idx = reduce(l, (unsigned) pa_atomic_load(&l->write_idx));
-
-    for (; n > 0 ; n--) {
-
-        _Y;
-
-        if (pa_atomic_ptr_cmpxchg(&cells[idx], NULL, p)) {
-
-            _Y;
-            pa_atomic_inc(&l->write_idx);
-
-            _Y;
-            pa_atomic_inc(&l->length);
-
-            return 0;
-        }
-
-        _Y;
-        idx = reduce(l, idx + 1);
+    elem = stack_pop(l, &l->empty);
+    if (elem == NULL) {
+        pa_log_warn("flist is full (don't worry)");
+        return -1;
     }
+    pa_atomic_ptr_store(&elem->ptr, p);
+    stack_push(l, &l->stored, elem);
 
-#ifdef PROFILE
-    if (len > N_EXTRA_SCAN)
-        pa_log_warn("Didn't  find free cell after %u iterations.", len);
-#endif
-
-    return -1;
+    return 0;
 }
 
-void* pa_flist_pop(pa_flist*l) {
-    unsigned idx, len, n;
-    pa_atomic_ptr_t *cells;
-
+void* pa_flist_pop(pa_flist *l) {
+    pa_flist_elem *elem;
+    void *ptr;
     pa_assert(l);
 
-    cells = PA_FLIST_CELLS(l);
-
-    n = len = (unsigned) pa_atomic_load(&l->length) + N_EXTRA_SCAN;
+    elem = stack_pop(l, &l->stored);
+    if (elem == NULL)
+        return NULL;
 
-    _Y;
-    idx = reduce(l, (unsigned) pa_atomic_load(&l->read_idx));
+    ptr = pa_atomic_ptr_load(&elem->ptr);
 
-    for (; n > 0 ; n--) {
-        void *p;
-
-        _Y;
-        p = pa_atomic_ptr_load(&cells[idx]);
-
-        if (p) {
-
-            _Y;
-            if (!pa_atomic_ptr_cmpxchg(&cells[idx], p, NULL))
-                continue;
-
-            _Y;
-            pa_atomic_inc(&l->read_idx);
-
-            _Y;
-            pa_atomic_dec(&l->length);
-
-            return p;
-        }
-
-        _Y;
-        idx = reduce(l, idx+1);
-    }
-
-#ifdef PROFILE
-    if (len > N_EXTRA_SCAN)
-        pa_log_warn("Didn't find used cell after %u iterations.", len);
-#endif
+    stack_push(l, &l->empty, elem);
 
-    return NULL;
+    return ptr;
 }
